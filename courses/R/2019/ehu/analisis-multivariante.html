<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> 9 Análisis multivariante | Introducción al software estadístico R</title>
  <meta name="description" content="Material del curso de Formación en R Euskaltel">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content=" 9 Análisis multivariante | Introducción al software estadístico R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Material del curso de Formación en R Euskaltel" />
  <meta name="github-repo" content="idaejin/xxx" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 9 Análisis multivariante | Introducción al software estadístico R" />
  
  <meta name="twitter:description" content="Material del curso de Formación en R Euskaltel" />
  

<meta name="author" content="Dae-Jin Lee &lt; dlee@bcamath.org &gt;">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="modelos-aditivos-generalizados.html">
<link rel="next" href="introduccion-a-las-redes-neuronales-artificiales.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso de formación en R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Información y pre-requisitos</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introducción al software estadístico <code>R</code></a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#algunas-cuestiones-a-tener-en-cuenta-sobre-r"><i class="fa fa-check"></i><b>2.1</b> Algunas cuestiones a tener en cuenta sobre <code>R</code></a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#rstudio"><i class="fa fa-check"></i><b>2.2</b> Rstudio</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#instalar-un-paquete-de-r"><i class="fa fa-check"></i><b>2.3</b> Instalar un paquete de <code>R</code></a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#empezando-con-r"><i class="fa fa-check"></i><b>2.4</b> Empezando con <code>R</code></a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#cargar-librerias-en-r"><i class="fa fa-check"></i><b>2.5</b> Cargar librerías en <code>R</code></a></li>
<li class="chapter" data-level="2.6" data-path="intro.html"><a href="intro.html#lectura-de-datos"><i class="fa fa-check"></i><b>2.6</b> Lectura de datos</a></li>
<li class="chapter" data-level="2.7" data-path="intro.html"><a href="intro.html#importar-datos"><i class="fa fa-check"></i><b>2.7</b> Importar datos</a></li>
<li class="chapter" data-level="2.8" data-path="intro.html"><a href="intro.html#exportar-datos"><i class="fa fa-check"></i><b>2.8</b> Exportar datos</a></li>
<li class="chapter" data-level="2.9" data-path="intro.html"><a href="intro.html#vectores"><i class="fa fa-check"></i><b>2.9</b> Vectores</a></li>
<li class="chapter" data-level="2.10" data-path="intro.html"><a href="intro.html#estadistica-basica"><i class="fa fa-check"></i><b>2.10</b> Estadística básica</a></li>
<li class="chapter" data-level="2.11" data-path="intro.html"><a href="intro.html#vectores-caracteres-y-variables-factor"><i class="fa fa-check"></i><b>2.11</b> Vectores caracteres y variables factor</a></li>
<li class="chapter" data-level="2.12" data-path="intro.html"><a href="intro.html#data-frames"><i class="fa fa-check"></i><b>2.12</b> Data frames</a><ul>
<li class="chapter" data-level="2.12.1" data-path="intro.html"><a href="intro.html#trabajando-con-data-frames"><i class="fa fa-check"></i><b>2.12.1</b> Trabajando con data frames</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="intro.html"><a href="intro.html#vectores-logicos"><i class="fa fa-check"></i><b>2.13</b> Vectores lógicos</a></li>
<li class="chapter" data-level="2.14" data-path="intro.html"><a href="intro.html#trabajando-con-vectores"><i class="fa fa-check"></i><b>2.14</b> Trabajando con vectores</a></li>
<li class="chapter" data-level="2.15" data-path="intro.html"><a href="intro.html#matrices-y-arrays"><i class="fa fa-check"></i><b>2.15</b> Matrices y arrays</a></li>
<li class="chapter" data-level="2.16" data-path="intro.html"><a href="intro.html#factores"><i class="fa fa-check"></i><b>2.16</b> Factores</a></li>
<li class="chapter" data-level="2.17" data-path="intro.html"><a href="intro.html#indexando-vectores-con-condiciones-logicas"><i class="fa fa-check"></i><b>2.17</b> Indexando vectores con condiciones lógicas</a></li>
<li class="chapter" data-level="2.18" data-path="intro.html"><a href="intro.html#valores-faltantes"><i class="fa fa-check"></i><b>2.18</b> Valores faltantes</a></li>
<li class="chapter" data-level="2.19" data-path="intro.html"><a href="intro.html#trabajando-con-data-frames-1"><i class="fa fa-check"></i><b>2.19</b> Trabajando con data frames</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic.html"><a href="basic.html"><i class="fa fa-check"></i><b>3</b> Análisis de datos básico en <code>R</code></a><ul>
<li class="chapter" data-level="3.1" data-path="basic.html"><a href="basic.html#graficos-sencillos"><i class="fa fa-check"></i><b>3.1</b> Gráficos sencillos</a></li>
<li class="chapter" data-level="3.2" data-path="basic.html"><a href="basic.html#scatterplots"><i class="fa fa-check"></i><b>3.2</b> Scatterplots</a></li>
<li class="chapter" data-level="3.3" data-path="basic.html"><a href="basic.html#mas-opciones-graficas"><i class="fa fa-check"></i><b>3.3</b> Más opciones gráficas</a></li>
<li class="chapter" data-level="3.4" data-path="basic.html"><a href="basic.html#tablas-de-clasificacion-cruzada-o-de-contigencia"><i class="fa fa-check"></i><b>3.4</b> Tablas de clasificación cruzada o de contigencia</a></li>
<li class="chapter" data-level="3.5" data-path="basic.html"><a href="basic.html#datos-cualitativos"><i class="fa fa-check"></i><b>3.5</b> Datos cualitativos</a></li>
<li class="chapter" data-level="3.6" data-path="basic.html"><a href="basic.html#datos-cuantitativos"><i class="fa fa-check"></i><b>3.6</b> Datos cuantitativos</a><ul>
<li class="chapter" data-level="3.6.1" data-path="basic.html"><a href="basic.html#distribuciones-de-frecuencias"><i class="fa fa-check"></i><b>3.6.1</b> Distribuciones de frecuencias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html"><i class="fa fa-check"></i><b>4</b> Introducción a la programación básica con <code>R</code></a><ul>
<li class="chapter" data-level="4.1" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#condicionales"><i class="fa fa-check"></i><b>4.1</b> Condicionales</a></li>
<li class="chapter" data-level="4.2" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#operadores-logicos"><i class="fa fa-check"></i><b>4.2</b> Operadores Lógicos</a></li>
<li class="chapter" data-level="4.3" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#if"><i class="fa fa-check"></i><b>4.3</b> <code>if</code></a></li>
<li class="chapter" data-level="4.4" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#ifelse"><i class="fa fa-check"></i><b>4.4</b> <code>ifelse</code></a></li>
<li class="chapter" data-level="4.5" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#loops-o-bucles"><i class="fa fa-check"></i><b>4.5</b> Loops o Bucles</a><ul>
<li class="chapter" data-level="4.5.1" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#for"><i class="fa fa-check"></i><b>4.5.1</b> <code>for</code></a></li>
<li class="chapter" data-level="4.5.2" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#while"><i class="fa fa-check"></i><b>4.5.2</b> <code>while</code></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#famila-de-funciones-apply"><i class="fa fa-check"></i><b>4.6</b> Famila de funciones <code>apply</code></a></li>
<li class="chapter" data-level="4.7" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#other-loops"><i class="fa fa-check"></i><b>4.7</b> Other Loops</a></li>
<li class="chapter" data-level="4.8" data-path="introduccion-a-la-programacion-basica-con-r.html"><a href="introduccion-a-la-programacion-basica-con-r.html#improving-speed-performance-of-loops"><i class="fa fa-check"></i><b>4.8</b> Improving Speed Performance of Loops</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html"><i class="fa fa-check"></i><b>5</b> Distribuciones de probabilidad en R</a><ul>
<li class="chapter" data-level="5.1" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#distribucion-binomial-binnp"><i class="fa fa-check"></i><b>5.1</b> Distribución binomial <span class="math inline">\(Bin(n,p)\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#distribucion-de-poisson-poislambda"><i class="fa fa-check"></i><b>5.2</b> Distribución de Poisson <span class="math inline">\(Pois(\lambda)\)</span></a><ul>
<li class="chapter" data-level="5.2.1" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#aproximacion-de-binomial-como-poisson"><i class="fa fa-check"></i><b>5.2.1</b> Aproximación de Binomial como Poisson</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#distribution-exponencial-explambda"><i class="fa fa-check"></i><b>5.3</b> Distribution Exponencial <span class="math inline">\(Exp(\lambda)\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#distribution-normal-mathcalnmusigma2"><i class="fa fa-check"></i><b>5.4</b> Distribution Normal <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="distribuciones-de-probabilidad-en-r.html"><a href="distribuciones-de-probabilidad-en-r.html#distribucion-uniforme-uab"><i class="fa fa-check"></i><b>5.5</b> Distribución Uniforme <span class="math inline">\(U(a,b)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html"><i class="fa fa-check"></i><b>6</b> Modelos lineales y análisis de la varianza</a><ul>
<li class="chapter" data-level="6.1" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#principios-de-la-modelizacion-estadistica"><i class="fa fa-check"></i><b>6.1</b> Principios de la modelización estadística</a><ul>
<li class="chapter" data-level="6.1.1" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#identificar-y-caracterizar-variables"><i class="fa fa-check"></i><b>6.1.1</b> Identificar y Caracterizar Variables</a></li>
<li class="chapter" data-level="6.1.2" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#tipos-de-variables-y-tipo-de-modelo"><i class="fa fa-check"></i><b>6.1.2</b> Tipos de variables y tipo de modelo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#el-modelo-lineal-general"><i class="fa fa-check"></i><b>6.2</b> El modelo lineal general</a></li>
<li class="chapter" data-level="6.3" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#definicion-de-modelos-en-r"><i class="fa fa-check"></i><b>6.3</b> Definición de modelos en <code>R</code></a><ul>
<li class="chapter" data-level="6.3.1" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#ejemplo-datos-de-precios-de-viviendas-en-boston"><i class="fa fa-check"></i><b>6.3.1</b> Ejemplo: Datos de precios de viviendas en Boston</a></li>
<li class="chapter" data-level="6.3.2" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>6.3.2</b> Regresión lineal múltiple</a></li>
<li class="chapter" data-level="6.3.3" data-path="modelos-lineales-y-analisis-de-la-varianza.html"><a href="modelos-lineales-y-analisis-de-la-varianza.html#modelos-con-interacciones"><i class="fa fa-check"></i><b>6.3.3</b> Modelos con interacciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regresion-logistica.html"><a href="regresion-logistica.html"><i class="fa fa-check"></i><b>7</b> Regresión logística</a><ul>
<li class="chapter" data-level="7.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-datos-de-credito-en-alemania-credit-scoring"><i class="fa fa-check"></i><b>7.1</b> Ejemplo: Datos de crédito en Alemania (<em>Credit scoring</em>)</a></li>
<li class="chapter" data-level="7.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-predecir-el-salario-de-los-trabajadores"><i class="fa fa-check"></i><b>7.2</b> Ejemplo: Predecir el salario de los trabajadores</a></li>
<li class="chapter" data-level="7.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-datos-de-los-supervivientes-del-titanic"><i class="fa fa-check"></i><b>7.3</b> Ejemplo: Datos de los supervivientes del Titanic</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html"><i class="fa fa-check"></i><b>8</b> Modelos Aditivos Generalizados</a><ul>
<li class="chapter" data-level="8.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#suavizado"><i class="fa fa-check"></i><b>8.1</b> Suavizado</a><ul>
<li class="chapter" data-level="8.1.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#regresion-polinomial"><i class="fa fa-check"></i><b>8.1.1</b> Regresión polinomial</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#gams"><i class="fa fa-check"></i><b>8.2</b> GAMs</a><ul>
<li class="chapter" data-level="8.2.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#ejemplo-salarios"><i class="fa fa-check"></i><b>8.2.1</b> Ejemplo: Salarios</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelos-semi-parametricos"><i class="fa fa-check"></i><b>8.3</b> Modelos semi-paramétricos</a><ul>
<li class="chapter" data-level="8.3.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#ejemplo"><i class="fa fa-check"></i><b>8.3.1</b> Ejemplo:</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#ejemplo-calidad-del-aire"><i class="fa fa-check"></i><b>8.4</b> Ejemplo: Calidad del Aire</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html"><i class="fa fa-check"></i><b>9</b> Análisis multivariante</a><ul>
<li class="chapter" data-level="9.1" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html#analisis-de-componentes-principales"><i class="fa fa-check"></i><b>9.1</b> Análisis de Componentes Principales</a></li>
<li class="chapter" data-level="9.2" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html#analisis-discriminante-lineal"><i class="fa fa-check"></i><b>9.2</b> Análisis Discriminante Lineal</a><ul>
<li class="chapter" data-level="9.2.1" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html#ejemplo-1"><i class="fa fa-check"></i><b>9.2.1</b> Ejemplo:</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html#k-medias"><i class="fa fa-check"></i><b>9.3</b> K-medias</a></li>
<li class="chapter" data-level="9.4" data-path="analisis-multivariante.html"><a href="analisis-multivariante.html#cluster-jerarquico"><i class="fa fa-check"></i><b>9.4</b> Cluster jerárquico</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html"><i class="fa fa-check"></i><b>10</b> Introduccion a las redes neuronales artificiales</a><ul>
<li class="chapter" data-level="10.1" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#arquitecturas-de-las-rna"><i class="fa fa-check"></i><b>10.1</b> Arquitecturas de las RNA</a></li>
<li class="chapter" data-level="10.2" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#ejemplo-datos-boston-housing"><i class="fa fa-check"></i><b>10.2</b> Ejemplo: datos Boston housing</a></li>
<li class="chapter" data-level="10.3" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#ajuste-de-la-red-neuronal"><i class="fa fa-check"></i><b>10.3</b> Ajuste de la red neuronal</a></li>
<li class="chapter" data-level="10.4" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#prediccion-con-una-red-neuronal"><i class="fa fa-check"></i><b>10.4</b> Predicción con una red neuronal</a></li>
<li class="chapter" data-level="10.5" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#evaluando-la-prediccion-mediante-validacion-cruzada"><i class="fa fa-check"></i><b>10.5</b> Evaluando la predicción mediante validación cruzada</a><ul>
<li class="chapter" data-level="10.5.1" data-path="introduccion-a-las-redes-neuronales-artificiales.html"><a href="introduccion-a-las-redes-neuronales-artificiales.html#comparacion-de-modelos-rna"><i class="fa fa-check"></i><b>10.5.1</b> Comparación de modelos RNA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="extras.html"><a href="extras.html"><i class="fa fa-check"></i><b>11</b> Extras</a><ul>
<li class="chapter" data-level="11.1" data-path="extras.html"><a href="extras.html#el-paquete-caret"><i class="fa fa-check"></i><b>11.1</b> El paquete <code>caret</code></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="http://idaejin.github.io/" target="blank">Mi página personal</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al software estadístico <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analisis-multivariante" class="section level1">
<h1><span class="header-section-number"> 9</span> Análisis multivariante</h1>
<div id="analisis-de-componentes-principales" class="section level2">
<h2><span class="header-section-number">9.1</span> Análisis de Componentes Principales</h2>
<p>El análisis de componentes principales (<em>principal component analysis</em>) o PCA es una de las técnicas de aprendizaje no supervisado, las cuales suelen aplicarse como parte del análisis exploratorio de los datos.</p>
<p>A diferencia de los métodos de aprendizaje supervisado, donde contamos con un grupo de variables o características (<span class="math inline">\(X=X_1,X_2,...,X_p\)</span>) medidas sobre un conjunto de observaciones <span class="math inline">\(n\)</span>, con la intención de obtener predicciones sobre una variable respuesta <span class="math inline">\(Y\)</span> asociada, en los no supervisados solo contamos con un número de variables de las cuales nos interesa conocer o de las que queremos extraer información, por ejemplo, sobre la existencia de subgrupos entre las variables u observaciones.</p>
<p>Una de las aplicaciones de PCA es la <strong>reducción de dimensionalidad</strong> (variables), perdiendo la menor cantidad de información (varianza) posible: cuando contamos con un gran número de variables cuantitativas posiblemente correlacionadas (indicativo de existencia de información redundante), PCA permite reducirlas a un número menor de variables transformadas (componentes principales) que expliquen gran parte de la variabilidad en los datos. Cada dimensión o componente principal generada por PCA será una combinación lineal de las variables originales, y serán además independientes o no correlacionadas entre sí.</p>
<p>Se utiliza para enfatizar la variación y sacar a relucir patrones fuertes en un conjunto de datos. A menudo se utiliza para hacer que los datos sean fáciles de explorar y visualizar. Vamos a realizar un PCA de los resultados obtenidos en la competición de heptatlón femenino de los Juegos Olímpicos de Seúl (1988).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(HSAUR2)</code></pre></div>
<pre><code>## Loading required package: tools</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;heptathlon&quot;</span>)
<span class="kw">head</span>(heptathlon)</code></pre></div>
<pre><code>##                     hurdles highjump  shot run200m longjump javelin
## Joyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66
## John (GDR)            12.85     1.80 16.23   23.65     6.71   42.56
## Behmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54
## Sablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78
## Choubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46
## Schulz (GDR)          13.75     1.83 13.50   24.65     6.33   42.82
##                     run800m score
## Joyner-Kersee (USA)  128.51  7291
## John (GDR)           126.12  6897
## Behmer (GDR)         124.20  6858
## Sablovskaite (URS)   132.24  6540
## Choubenkova (URS)    127.90  6540
## Schulz (GDR)         125.79  6411</code></pre>
<p>Recodificamos las pruebas relativas a 3 carreras <code>hurdles</code>, <code>run200m</code> y <code>run800m</code>, restando el valor más alto en cada carrera, cada uno de los 35 tiempos de los atletas.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heptathlon<span class="op">$</span>hurdles &lt;-<span class="st"> </span><span class="kw">max</span>(heptathlon<span class="op">$</span>hurdles) <span class="op">-</span><span class="st"> </span>heptathlon<span class="op">$</span>hurdles
heptathlon<span class="op">$</span>run200m &lt;-<span class="st"> </span><span class="kw">max</span>(heptathlon<span class="op">$</span>run200m) <span class="op">-</span><span class="st"> </span>heptathlon<span class="op">$</span>run200m
heptathlon<span class="op">$</span>run800m &lt;-<span class="st"> </span><span class="kw">max</span>(heptathlon<span class="op">$</span>run800m) <span class="op">-</span><span class="st"> </span>heptathlon<span class="op">$</span>run800m</code></pre></div>
<p>Diagrama de dispersion</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">score &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">colnames</span>(heptathlon) <span class="op">==</span><span class="st"> &quot;score&quot;</span>)
<span class="kw">plot</span>(heptathlon[,<span class="op">-</span>score])</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/hep-08-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Matriz de correlación</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">cor</span>(heptathlon[,<span class="op">-</span>score]),<span class="dv">3</span>)</code></pre></div>
<pre><code>##          hurdles highjump  shot run200m longjump javelin run800m
## hurdles    1.000    0.811 0.651   0.774    0.912   0.008   0.779
## highjump   0.811    1.000 0.441   0.488    0.782   0.002   0.591
## shot       0.651    0.441 1.000   0.683    0.743   0.269   0.420
## run200m    0.774    0.488 0.683   1.000    0.817   0.333   0.617
## longjump   0.912    0.782 0.743   0.817    1.000   0.067   0.700
## javelin    0.008    0.002 0.269   0.333    0.067   1.000  -0.020
## run800m    0.779    0.591 0.420   0.617    0.700  -0.020   1.000</code></pre>
<p>La matriz de resultados confirma que la gran mayoría de las correlaciones entre las pruebas son positivas, con una alta correlación entre el salto de longitud y el salto de longitud (<code>longjump</code>) y los 100m vallas (<code>hurdles</code>). A algunos les gusta menos el salto de altura (<code>highjump</code>) y el disparo (<code>shot</code>) y la jabalina (<code>javelin</code>) que tiene una correlación cercana a cero con el resto de las pruebas.</p>
<p>Una posible explicación para este resultado podría ser que el entrenamiento para las otras 6 pruebas no añade mucho a la prueba de jabalina, que es una prueba más técnica.</p>
<p>Puede observarse que existe un valor atípico en casi todas las pruebas que corresponde a un atleta <code>Launa (PNG)</code> de Papúa Nueva Guinea - eliminaremos esta observación para ver si la matriz de correlación es significativamente diferente:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heptathlon &lt;-<span class="st"> </span>heptathlon[<span class="op">-</span><span class="kw">which</span>(<span class="kw">rownames</span>(heptathlon)<span class="op">==</span><span class="st">&quot;Launa (PNG)&quot;</span>),]
<span class="kw">plot</span>(heptathlon[,<span class="op">-</span>score])</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/hepta-08a-1.png" width="672" /></p>
<p>Eliminando al atleta de Papúa Nueva Guinea, las correlaciones cambian sustancialmente y en el diagrama de dispersión matricial no se observan valores extremos.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">cor</span>(heptathlon[,<span class="op">-</span>score]),<span class="dv">3</span>)</code></pre></div>
<pre><code>##          hurdles highjump  shot run200m longjump javelin run800m
## hurdles    1.000    0.582 0.767   0.830    0.889   0.332   0.559
## highjump   0.582    1.000 0.465   0.391    0.663   0.348   0.152
## shot       0.767    0.465 1.000   0.669    0.784   0.343   0.408
## run200m    0.830    0.391 0.669   1.000    0.811   0.471   0.573
## longjump   0.889    0.663 0.784   0.811    1.000   0.287   0.523
## javelin    0.332    0.348 0.343   0.471    0.287   1.000   0.256
## run800m    0.559    0.152 0.408   0.573    0.523   0.256   1.000</code></pre>
<p>Para realizar el PCA, partiremos de la matriz de correlación, ya que las 7 pruebas se miden en diferentes escalas (metros, segundos). Este procedimiento se denomina PCA normalizado (<code>scale=TRUE</code> en la función <code>prcomp</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?prcomp
heptathlon_pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(heptathlon[,<span class="op">-</span>score],<span class="dt">scale=</span><span class="ot">TRUE</span>)
<span class="kw">head</span>(heptathlon_pca,<span class="dv">5</span>)</code></pre></div>
<pre><code>## $sdev
## [1] 2.0793370 0.9481532 0.9109016 0.6831967 0.5461888 0.3374549 0.2620420
## 
## $rotation
##                 PC1         PC2        PC3         PC4         PC5
## hurdles  -0.4503876  0.05772161 -0.1739345  0.04840598 -0.19889364
## highjump -0.3145115 -0.65133162 -0.2088272 -0.55694554  0.07076358
## shot     -0.4024884 -0.02202088 -0.1534709  0.54826705  0.67166466
## run200m  -0.4270860  0.18502783  0.1301287  0.23095946 -0.61781764
## longjump -0.4509639 -0.02492486 -0.2697589 -0.01468275 -0.12151793
## javelin  -0.2423079 -0.32572229  0.8806995  0.06024757  0.07874396
## run800m  -0.3029068  0.65650503  0.1930020 -0.57418128  0.31880178
##                  PC6         PC7
## hurdles   0.84665086 -0.06961672
## highjump -0.09007544  0.33155910
## shot     -0.09886359  0.22904298
## run200m  -0.33279359  0.46971934
## longjump -0.38294411 -0.74940781
## javelin   0.07193437 -0.21108138
## run800m  -0.05217664  0.07718616
## 
## $center
##   hurdles  highjump      shot   run200m  longjump   javelin   run800m 
##  2.687500  1.793750 13.173333  2.023750  6.205417 41.278333 28.516667 
## 
## $scale
##    hurdles   highjump       shot    run200m   longjump    javelin 
## 0.51456398 0.05232112 1.49714995 0.93676972 0.40165938 3.46870690 
##    run800m 
## 6.14724800 
## 
## $x
##                              PC1         PC2          PC3          PC4
## Joyner-Kersee (USA) -4.757530189 -0.13986143 -0.006040526  0.293416339
## John (GDR)          -3.147943402  0.94859029 -0.243919842  0.549171385
## Behmer (GDR)        -2.926184760  0.69534239  0.622293440 -0.554744912
## Sablovskaite (URS)  -1.288135516  0.17900713  0.250632380  0.637174187
## Choubenkova (URS)   -1.503450994  0.96177329  1.780588549  0.784035325
## Schulz (GDR)        -0.958467101  0.35121643  0.413086366 -1.113546938
## Fleming (AUS)       -0.953445060  0.49982537 -0.265135015 -0.140202490
## Greiner (USA)       -0.633239267  0.37592917 -1.140338594  0.142558348
## Lajbnerova (CZE)    -0.381571974 -0.71213459 -0.068395353  0.087212735
## Bouraga (URS)       -0.522322004  0.77688861 -0.481071429  0.283745698
## Wijnsma (HOL)       -0.217701500 -0.23369645 -1.154221444 -1.260128609
## Dimitrova (BUL)     -1.075984276  0.51552998 -0.312458252 -0.127032432
## Scheider (SWI)       0.003014986 -1.44688825  1.582739069 -1.254415325
## Braun (FRG)          0.109183759 -1.63595645  0.469577294  0.362580442
## Ruotsalainen (FIN)   0.208868056 -0.68866173  1.152140223 -0.112914470
## Yuping (CHN)         0.232507119 -1.95999641 -1.541230813  0.598325122
## Hagger (GB)          0.659520046 -0.08775813 -1.796509771 -0.182375000
## Brown (USA)          0.756854602 -2.04292201  0.451506018  0.476926314
## Mulliner (GB)        1.880932819  0.91530324 -0.359311801  0.799619094
## Hautenauve (BEL)     1.828170404  0.72629699 -1.048640439 -0.711793161
## Kytola (FIN)         2.118203163  0.39921397  0.190158154 -0.788445056
## Geremias (BRA)       2.770706272  0.03463584  0.170274969  1.385562494
## Hui-Ing (TAI)        3.901166920  1.20175472  0.943677497 -0.002429122
## Jeong-Mi (KOR)       3.896847898  0.36656804  0.390599321 -0.152299968
##                             PC5         PC6         PC7
## Joyner-Kersee (USA) -0.36183307 -0.27050283 -0.47587527
## John (GDR)           0.75364464  0.37770017 -0.05172711
## Behmer (GDR)        -0.19035037 -0.25780287  0.11054960
## Sablovskaite (URS)   0.60362153 -0.21575716  0.53075152
## Choubenkova (URS)    0.58969949  0.08014332 -0.30081842
## Schulz (GDR)         0.71483887 -0.25436956  0.03838796
## Fleming (AUS)       -0.86581530  0.03691813  0.23005943
## Greiner (USA)        0.20807431 -0.14236240 -0.06374657
## Lajbnerova (CZE)     0.67727618  0.25014881  0.35555639
## Bouraga (URS)       -1.18784299  0.39881271  0.19712215
## Wijnsma (HOL)        0.37497195 -0.20267731  0.17459647
## Dimitrova (BUL)     -0.91992929  0.26727067  0.21111846
## Scheider (SWI)      -0.20526249  0.17597425 -0.03915701
## Braun (FRG)         -0.14712208  0.26134199 -0.01334416
## Ruotsalainen (FIN)  -0.31539746  0.18351622 -0.14127555
## Yuping (CHN)         0.17451428 -0.50175724  0.04999374
## Hagger (GB)         -0.05104049  0.55058471 -0.46388534
## Brown (USA)         -0.38154294 -0.26606429 -0.11099445
## Mulliner (GB)       -0.06942955 -0.73259727 -0.31281502
## Hautenauve (BEL)     0.14092347  0.06933542 -0.07548638
## Kytola (FIN)         0.41815113 -0.03363651  0.12143219
## Geremias (BRA)       0.28541366  0.38083979  0.34574480
## Hui-Ing (TAI)       -0.67080776 -0.52756760  0.09436975
## Jeong-Mi (KOR)       0.42524426  0.37250885 -0.41055719</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a1 &lt;-<span class="st"> </span>heptathlon_pca<span class="op">$</span>rotation[,<span class="dv">1</span>]</code></pre></div>
<p><code>summary</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(heptathlon_pca)</code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6
## Standard deviation     2.0793 0.9482 0.9109 0.68320 0.54619 0.33745
## Proportion of Variance 0.6177 0.1284 0.1185 0.06668 0.04262 0.01627
## Cumulative Proportion  0.6177 0.7461 0.8646 0.93131 0.97392 0.99019
##                            PC7
## Standard deviation     0.26204
## Proportion of Variance 0.00981
## Cumulative Proportion  1.00000</code></pre>
<p>Comprobamos la variabilidad explicada:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(heptathlon_pca<span class="op">$</span>x)</code></pre></div>
<pre><code>##                            PC1        PC2          PC3        PC4
## Joyner-Kersee (USA) -4.7575302 -0.1398614 -0.006040526  0.2934163
## John (GDR)          -3.1479434  0.9485903 -0.243919842  0.5491714
## Behmer (GDR)        -2.9261848  0.6953424  0.622293440 -0.5547449
## Sablovskaite (URS)  -1.2881355  0.1790071  0.250632380  0.6371742
## Choubenkova (URS)   -1.5034510  0.9617733  1.780588549  0.7840353
## Schulz (GDR)        -0.9584671  0.3512164  0.413086366 -1.1135469
##                            PC5         PC6         PC7
## Joyner-Kersee (USA) -0.3618331 -0.27050283 -0.47587527
## John (GDR)           0.7536446  0.37770017 -0.05172711
## Behmer (GDR)        -0.1903504 -0.25780287  0.11054960
## Sablovskaite (URS)   0.6036215 -0.21575716  0.53075152
## Choubenkova (URS)    0.5896995  0.08014332 -0.30081842
## Schulz (GDR)         0.7148389 -0.25436956  0.03838796</code></pre>
<p>Gráficamente, se observa que el primer componente importante es el más dominante.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(heptathlon_pca)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/pca-hepta-1.png" width="672" style="display: block; margin: auto;" /> El <em>biplot</em> es una representación gráfica de datos multivariantes. Así como una gráfica de dispersión muestra la distribución combinada de dos variables, una <strong>biplot</strong> representa tres o más variables.</p>
<p>Si ordenamos de mayor a menor la variable ``puntuación’’ tenemos las tres medallas de oro, plata y bronce.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(heptathlon[<span class="kw">order</span>(heptathlon<span class="op">$</span>score,<span class="dt">decreasing =</span> <span class="ot">TRUE</span>),],<span class="dv">3</span>)</code></pre></div>
<pre><code>##                     hurdles highjump  shot run200m longjump javelin
## Joyner-Kersee (USA)    3.73     1.86 15.80    4.05     7.27   45.66
## John (GDR)             3.57     1.80 16.23    2.96     6.71   42.56
## Behmer (GDR)           3.22     1.83 14.20    3.51     6.68   44.54
##                     run800m score
## Joyner-Kersee (USA)   34.92  7291
## John (GDR)            37.31  6897
## Behmer (GDR)          39.23  6858</code></pre>
<p> </p>
<p>El biplot, nos muestra los atletas proyectados en sus dos primeros componentes principales, pero también las flechas nos dan información sobre las varianzas y covarianzas de las variables (direcciones de máxima variabilidad).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(heptathlon_pca)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/biplot-08a-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>Por ejemplo, el ganador <code>Joyner-Kersee (USA)</code> acumula puntuaciones más altas en el <code>longjump</code>,<code>hurdlesp</code> y <code>run200m</code>.</p>
<p> </p>
<p>Podemos analizar la correlación entre la puntuación de la variable y PC1. Esto indica que la correlación es muy negativa y muy fuerte con el <code>score</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(heptathlon<span class="op">$</span>score, heptathlon_pca<span class="op">$</span>x[,<span class="dv">1</span>])</code></pre></div>
<pre><code>## [1] -0.9931168</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(heptathlon<span class="op">$</span>score, heptathlon_pca<span class="op">$</span>x[,<span class="dv">1</span>])</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/hepta-08b-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>objeto <code>prcomp</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(heptathlon_pca)</code></pre></div>
<pre><code>## [1] &quot;prcomp&quot;</code></pre>
<p>Podemos usar <code>predict</code>, ej.:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.athlete &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">t</span>(<span class="kw">as.vector</span>(<span class="kw">c</span>(<span class="fl">3.5</span>,<span class="dv">2</span>,<span class="dv">13</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">41</span>,<span class="dv">33</span>))))
<span class="kw">colnames</span>(new.athlete) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;hurdles&quot;</span>,<span class="st">&quot;highjump&quot;</span>,
                           <span class="st">&quot;shot&quot;</span>,<span class="st">&quot;run200m&quot;</span>,<span class="st">&quot;longjump&quot;</span>,
                           <span class="st">&quot;javelin&quot;</span>,<span class="st">&quot;run800m&quot;</span>)
<span class="kw">rownames</span>(new.athlete) &lt;-<span class="st"> &quot;Mrs XYZ (XXX)&quot;</span>
pp&lt;-<span class="kw">predict</span>(heptathlon_pca,<span class="dt">newdata =</span> new.athlete)
pp</code></pre></div>
<pre><code>##                     PC1       PC2       PC3       PC4       PC5        PC6
## Mrs XYZ (XXX) -4.354879 -1.430366 -1.130194 -1.901377 -2.089963 -0.8654821
##                    PC7
## Mrs XYZ (XXX) 1.253643</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(heptathlon_pca)
<span class="kw">text</span>(pp[,<span class="dv">1</span>],pp[,<span class="dv">2</span>],<span class="st">&quot;Mrs XYZ (XXX)&quot;</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">cex=</span>.<span class="dv">65</span>)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/biplot-08b-1.png" width="1440" /></p>
</div>
<div id="analisis-discriminante-lineal" class="section level2">
<h2><span class="header-section-number">9.2</span> Análisis Discriminante Lineal</h2>
<p>El análisis discriminante lineal (LDA - <em>linear discriminant analysis</em>) y el discriminante lineal relacionado de Fisher son métodos utilizados en la estadística, el reconocimiento de patrones y el aprendizaje automático para encontrar una combinación lineal de características que caracteriza o separa dos o más clases de objetos o eventos. La combinación resultante puede utilizarse como clasificador lineal o, más comúnmente, para la reducción de la dimensionalidad antes de su posterior clasificación.</p>
<p>Es un método alternativo más adecuado a la regresión logística cuando la variable cualitativa tiene más de dos niveles (<span class="math inline">\(K \geq 2\)</span>). Supone también un modelo más estable cuando el tamaño muestral <span class="math inline">\(n\)</span> es pequeño y la distribución de los predictores es aproximadamente normal en cada una de sus clases. El propósito del LDA es encontrar la combinación lineal de las variables originales que permita la mejor separación entre grupos de un set de datos. El LDA está basado en el clasificador Bayesiano.</p>
<p>Ilustración</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="op">-</span><span class="dv">4</span><span class="op">:</span><span class="dv">4</span>), <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span>
<span class="co"># distribución de densidad de k = 1</span>
<span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm,
<span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="op">-</span><span class="fl">1.25</span>, <span class="dt">sd =</span> <span class="dv">1</span>),
<span class="dt">color =</span> <span class="st">&quot;green3&quot;</span>) <span class="op">+</span>
<span class="co"># distribución de densidad de k = 2</span>
<span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm,
<span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="fl">1.25</span>, <span class="dt">sd =</span> <span class="dv">1</span>),
<span class="dt">color =</span> <span class="st">&quot;firebrick&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="co"># límite de decisión de Bayes</span>
<span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>,
<span class="dt">linetype =</span> <span class="st">&quot;longdash&quot;</span>) <span class="op">+</span>
<span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/LDA-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>En este caso, el clasificador de Bayes asignará la observación a la clase <span class="math inline">\(k = 1\)</span> (verde) si <span class="math inline">\(x &lt; 0\)</span>, y a la clase <span class="math inline">\(k = 2\)</span> (rojo) si <span class="math inline">\(x &gt; 0\)</span>.</p>
<div id="ejemplo-1" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Ejemplo:</h3>
<p>Vamos a predecir si el rendimiento del combustible (gas mileage) de un automóvil es alto o bajo en función del resto de predictores del set de datos.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)
<span class="kw">head</span>(Auto, <span class="dv">3</span>)</code></pre></div>
<pre><code>##   mpg cylinders displacement horsepower weight acceleration year origin
## 1  18         8          307        130   3504         12.0   70      1
## 2  15         8          350        165   3693         11.5   70      1
## 3  18         8          318        150   3436         11.0   70      1
##                        name
## 1 chevrolet chevelle malibu
## 2         buick skylark 320
## 3        plymouth satellite</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?Auto

<span class="kw">attach</span>(Auto) <span class="co"># Vector de “0”s (rendimiento bajo) con la misma longitud que la variable mpg </span></code></pre></div>
<pre><code>## The following object is masked from package:ggplot2:
## 
##     mpg</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mpg01 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(mpg))
<span class="co"># Sustitución de “0”s por “1”s (rendimiento alto) si mpg &gt; mediana(mpg) </span>
mpg01[mpg <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(mpg)] &lt;-<span class="st"> </span><span class="dv">1</span>
Auto &lt;-<span class="st"> </span><span class="kw">data.frame</span>(Auto, mpg01)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Correlación entre variables (excluyendo la variable cualitativa “name”)</span>
<span class="kw">cor</span>(Auto[, <span class="op">-</span><span class="dv">9</span>], <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>)</code></pre></div>
<pre><code>##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
## origin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054
## mpg01         0.8369392 -0.7591939   -0.7534766 -0.6670526 -0.7577566
##              acceleration       year     origin      mpg01
## mpg             0.4233285  0.5805410  0.5652088  0.8369392
## cylinders      -0.5046834 -0.3456474 -0.5689316 -0.7591939
## displacement   -0.5438005 -0.3698552 -0.6145351 -0.7534766
## horsepower     -0.6891955 -0.4163615 -0.4551715 -0.6670526
## weight         -0.4168392 -0.3091199 -0.5850054 -0.7577566
## acceleration    1.0000000  0.2903161  0.2127458  0.3468215
## year            0.2903161  1.0000000  0.1815277  0.4299042
## origin          0.2127458  0.1815277  1.0000000  0.5136984
## mpg01           0.3468215  0.4299042  0.5136984  1.0000000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))
<span class="kw">boxplot</span>(cylinders <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Cylinders vs mpg01&quot;</span>)
<span class="kw">boxplot</span>(displacement <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Displacement vs mpg01&quot;</span>)
<span class="kw">boxplot</span>(horsepower <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Horsepower vs mpg01&quot;</span>)
<span class="kw">boxplot</span>(weight <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Weight vs mpg01&quot;</span>)
<span class="kw">boxplot</span>(acceleration <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Acceleration vs mpg01&quot;</span>)
<span class="kw">boxplot</span>(year <span class="op">~</span><span class="st"> </span>mpg01, <span class="dt">data =</span> Auto, <span class="dt">main =</span> <span class="st">&quot;Year vs mpg01&quot;</span>)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/auto-boxplots-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Representación de distribución en histogramas:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(gridExtra)
p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> Auto, <span class="kw">aes</span>(<span class="dt">x =</span> displacement)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position =</span> <span class="st">&quot;identity&quot;</span>,
<span class="dt">alpha =</span> <span class="fl">0.5</span>,
<span class="kw">aes</span>(<span class="dt">fill =</span> <span class="kw">as.factor</span>(mpg01)))<span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">fill =</span> <span class="st">&quot;mpg01&quot;</span>)
p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> Auto, <span class="kw">aes</span>(<span class="dt">x =</span> horsepower)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position =</span> <span class="st">&quot;identity&quot;</span>,
<span class="dt">alpha =</span> <span class="fl">0.5</span>,
<span class="kw">aes</span>(<span class="dt">fill =</span> <span class="kw">as.factor</span>(mpg01)))<span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">fill =</span> <span class="st">&quot;mpg01&quot;</span>)
p3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data =</span> Auto, <span class="kw">aes</span>(<span class="dt">x =</span> weight)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position =</span> <span class="st">&quot;identity&quot;</span>,
<span class="dt">alpha =</span> <span class="fl">0.5</span>,
<span class="kw">aes</span>(<span class="dt">fill =</span> <span class="kw">as.factor</span>(mpg01)))<span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">fill =</span> <span class="st">&quot;mpg01&quot;</span>)
<span class="kw">grid.arrange</span>(p1, p2, p3)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="cursoR_Euskaltel_files/figure-html/auto-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Cálculo de la función discriminante</strong></p>
<p>Primero dividiremos el set de datos en entrenamiento (80%) para ajustar el modelo, y en test (20%) para evaluarlo.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
entrenamiento &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> <span class="kw">nrow</span>(Auto), <span class="dt">size =</span> <span class="kw">nrow</span>(Auto)<span class="op">*</span><span class="fl">0.8</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>) <span class="co"># Subgrupo de datos de entrenamiento</span>
Auto.train &lt;-<span class="st"> </span>Auto[entrenamiento,]
<span class="co"># Subgrupo de datos de test</span>
Auto.test &lt;-<span class="st"> </span>Auto[<span class="op">-</span>entrenamiento,]</code></pre></div>
<p>Función <code>lda</code> en la librería <code>MASS</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
<span class="co"># Modelo LDA con los datos de entrenamiento</span>
modelo.lda &lt;-<span class="st"> </span><span class="kw">lda</span>(<span class="dt">formula =</span> mpg01 <span class="op">~</span><span class="st"> </span>cylinders <span class="op">+</span><span class="st"> </span>displacement <span class="op">+</span><span class="st"> </span>horsepower <span class="op">+</span><span class="st"> </span>weight , <span class="dt">data =</span> Auto.train)

modelo.lda</code></pre></div>
<pre><code>## Call:
## lda(mpg01 ~ cylinders + displacement + horsepower + weight, data = Auto.train)
## 
## Prior probabilities of groups:
##        0        1 
## 0.514377 0.485623 
## 
## Group means:
##   cylinders displacement horsepower   weight
## 0  6.677019     269.8323  128.71429 3601.379
## 1  4.210526     117.4145   78.57895 2350.526
## 
## Coefficients of linear discriminants:
##                        LD1
## cylinders    -0.4183687107
## displacement -0.0017457149
## horsepower    0.0028180950
## weight       -0.0009283838</code></pre>
</div>
</div>
<div id="k-medias" class="section level2">
<h2><span class="header-section-number">9.3</span> K-medias</h2>
<p>K-means Clustering es un algoritmo de aprendizaje no supervisado que intenta agrupar datos basados en su similitud. El aprendizaje sin supervisión significa que no hay resultados que predecir, y el algoritmo sólo trata de encontrar patrones en los datos. En k-means clustering, tenemos que especificar el número de clusters en los que queremos que se agrupen los datos. El algoritmo asigna aleatoriamente cada observación a un cúmulo, y encuentra el centroide de cada cúmulo. Luego, el algoritmo itera a través de dos pasos:</p>
<ol style="list-style-type: decimal">
<li><p>Reasigne los puntos de datos al cluster cuyo centroide está más cerca.</p></li>
<li><p>Calcular el nuevo centroide de cada cúmulo.</p></li>
</ol>
<p>Estos dos pasos se repiten hasta que la variación dentro del conglomerado no pueda reducirse más. La variación dentro del conglomerado se calcula como la suma de la distancia euclídea entre los puntos de datos y sus respectivos centros de conglomerados.</p>
<p><strong>Ejemplo:</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rattle)</code></pre></div>
<pre><code>## Rattle: A free graphical interface for data science with R.
## Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.
## Type &#39;rattle()&#39; to shake, rattle, and roll your data.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(wine)
<span class="kw">head</span>(wine)</code></pre></div>
<pre><code>##   Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids
## 1    1   14.23  1.71 2.43       15.6       127    2.80       3.06
## 2    1   13.20  1.78 2.14       11.2       100    2.65       2.76
## 3    1   13.16  2.36 2.67       18.6       101    2.80       3.24
## 4    1   14.37  1.95 2.50       16.8       113    3.85       3.49
## 5    1   13.24  2.59 2.87       21.0       118    2.80       2.69
## 6    1   14.20  1.76 2.45       15.2       112    3.27       3.39
##   Nonflavanoids Proanthocyanins Color  Hue Dilution Proline
## 1          0.28            2.29  5.64 1.04     3.92    1065
## 2          0.26            1.28  4.38 1.05     3.40    1050
## 3          0.30            2.81  5.68 1.03     3.17    1185
## 4          0.24            2.18  7.80 0.86     3.45    1480
## 5          0.39            1.82  4.32 1.04     2.93     735
## 6          0.34            1.97  6.75 1.05     2.85    1450</code></pre>
<p>Siempre se recomienda estandarizar las variables</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine.stand &lt;-<span class="st"> </span><span class="kw">scale</span>(wine[<span class="op">-</span><span class="dv">1</span>])  <span class="co"># To standarize the variables</span>
<span class="co"># K-Means</span>
k.means.fit &lt;-<span class="st"> </span><span class="kw">kmeans</span>(wine.stand, <span class="dv">3</span>) <span class="co"># k = 3</span>
<span class="kw">attributes</span>(k.means.fit)</code></pre></div>
<pre><code>## $names
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;      
## 
## $class
## [1] &quot;kmeans&quot;</code></pre>
<p>Una pregunta fundamental es cómo determinar el valor del parámetro <span class="math inline">\(k\)</span>. Si consideramos el porcentaje de varianza explicado en función del número de grupos:</p>
<p>Uno debe elegir un número de grupos para que la incorporación de otro grupo no dé un mejor modelado de los datos. Más precisamente, si el porcentaje de varianza explicado por los clusters se traza de acuerdo al número de grupos, los primeros clusters añadirán mucha información (explican mucha varianza), pero en algún momento la ganancia marginal disminuirá, dando un ángulo en el gráfico. En este punto se elige el número de racimos, de ahí el “criterio del codo”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wssplot &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">nc=</span><span class="dv">15</span>, <span class="dt">seed=</span><span class="dv">1234</span>){
  wss &lt;-<span class="st"> </span>(<span class="kw">nrow</span>(data)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span><span class="kw">sum</span>(<span class="kw">apply</span>(data,<span class="dv">2</span>,var))
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>nc){
    <span class="kw">set.seed</span>(seed)
    wss[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">kmeans</span>(data, <span class="dt">centers=</span>i)<span class="op">$</span>withinss)}
  <span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span>nc, wss, <span class="dt">type=</span><span class="st">&quot;b&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Number of Clusters&quot;</span>,
       <span class="dt">ylab=</span><span class="st">&quot;Within groups sum of squares&quot;</span>)}

<span class="kw">wssplot</span>(wine.stand, <span class="dt">nc=</span><span class="dv">6</span>)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/wsplot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>La <code>library(cluster)</code> permite representar los datos en dos dimensiones:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cluster)
<span class="kw">clusplot</span>(wine.stand, k.means.fit<span class="op">$</span>cluster,
         <span class="dt">main=</span><span class="st">&#39;2D representation of the Cluster solution&#39;</span>,
         <span class="dt">color=</span><span class="ot">TRUE</span>, <span class="dt">shade=</span><span class="ot">TRUE</span>,
         <span class="dt">labels=</span><span class="dv">2</span>, <span class="dt">lines=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/clust-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Sabiendo que hay tres tipos de <code>wine$Type</code> wines, podemos calcular la matriz de confusión.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(wine<span class="op">$</span>Type)</code></pre></div>
<pre><code>## 
##  1  2  3 
## 59 71 48</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(wine[,<span class="dv">1</span>],k.means.fit<span class="op">$</span>cluster)</code></pre></div>
<pre><code>##    
##      1  2  3
##   1  0 59  0
##   2  3  3 65
##   3 48  0  0</code></pre>
</div>
<div id="cluster-jerarquico" class="section level2">
<h2><span class="header-section-number">9.4</span> Cluster jerárquico</h2>
<p>Los métodos jerárquicos utilizan una matriz de distancia como entrada para el algoritmo de agrupación. La elección de una métrica apropiada influirá en la forma de los cúmulos, ya que algunos elementos pueden estar próximos entre sí de acuerdo a una distancia y más separados de acuerdo a otra.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(wine.stand, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>) <span class="co"># Euclidean distance matrix.</span></code></pre></div>
<p>El criterio de desviación mínima de Ward minimiza la desviación total dentro del grupo de empresas.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">H.fit &lt;-<span class="st"> </span><span class="kw">hclust</span>(d, <span class="dt">method=</span><span class="st">&quot;ward&quot;</span>)</code></pre></div>
<pre><code>## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">class</span>(H.fit)</code></pre></div>
<pre><code>## [1] &quot;hclust&quot;</code></pre>
<p>La opción <code>plot</code> devuelve un <code>hclust</code> que muestra el dendograma:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(H.fit) <span class="co"># display dendogram</span>
groups &lt;-<span class="st"> </span><span class="kw">cutree</span>(H.fit, <span class="dt">k=</span><span class="dv">3</span>) <span class="co"># cut tree into 5 clusters</span>
<span class="co"># draw dendogram with red borders around the 5 clusters</span>
<span class="kw">rect.hclust</span>(H.fit, <span class="dt">k=</span><span class="dv">3</span>, <span class="dt">border=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="cursoR_Euskaltel_files/figure-html/dendo-1.png" width="1152" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(wine[,<span class="dv">1</span>],groups)</code></pre></div>
<pre><code>##    groups
##      1  2  3
##   1 58  1  0
##   2  7 58  6
##   3  0  0 48</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modelos-aditivos-generalizados.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduccion-a-las-redes-neuronales-artificiales.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": ["cursoR_Euskaltel.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
